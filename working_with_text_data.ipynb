{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb3b878-5661-4a90-8956-532ad6d0ecd3",
   "metadata": {},
   "source": [
    " It’s common to process millions of articles and hundreds of thousands\n",
    "of books—many gigabytes of text—when working with LLMs. However, for\n",
    "educational purposes, it’s sufficient to work with smaller text samples like a\n",
    "single book to illustrate the main ideas behind the text processing steps and\n",
    "to make it possible to run it in a reasonable time on consumer hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "702506df-94e4-49c8-aef2-821b3ec86cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of charactor:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "          raw_text = f.read()\n",
    "print (\"Total number of charactor: \", len(raw_text))\n",
    "\n",
    "# Print the first 100 charactores in text\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2206ca71-22d7-486b-a34f-54ed685f9f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Python’s regular expression library re\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "121355be-4e20-4f57-a77c-7264f14e44c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hellow,', ' ', 'World.', ' ', 'This', ' ', 'is', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hellow, World. This is test\"\n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3456409f-0916-487a-b7e7-e83d0ef57f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hellow', ', ', 'World', '. ', 'This', ' ', 'is', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "# words and punctuation characters are now separate\n",
    "result = re.split(r'([, .] |\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b408f11d-f8e6-40ea-b528-88dc2c17acb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hellow', ', ', 'World', '. ', 'This', 'is', 'test']\n"
     ]
    }
   ],
   "source": [
    "# A small remaining problem is that the list still includes whitespace characters\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3cd0e1f-90a2-4ed1-a914-3013515439e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476f4e3-2dee-4890-8dbc-b4ec8c176107",
   "metadata": {},
   "source": [
    "# Now that we have a basic tokenizer working, let’s apply it to Edith Wharton’s entire short story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "300569a5-8256-439d-ace5-c035207c3507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4092\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"\\' ()] | --|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57c85c38-3c61-459c-bc48-85a3e9c65fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius--though', 'a', 'good', 'fellow', 'enough--so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his']\n"
     ]
    }
   ],
   "source": [
    "# Let’s print the first 30 tokens for a quick visual check:\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43de84a-ee6e-411f-b64b-6647d2a27277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef21c6-c637-45d3-ae23-a0bb7a4af6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
